{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import  division\n",
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import linalg, special\n",
    "from numpy.linalg import norm\n",
    "import statsmodels.api as st\n",
    "import family\n",
    "from scipy.sparse import linalg as sp_linalg\n",
    "from sklearn import datasets, neighbors, linear_model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from optimize import newton_cg\n",
    "from scipy import optimize\n",
    "from scipy.misc import logsumexp\n",
    "import statsmodels.regression.linear_model as lim\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import datetime\n",
    "import pytz\n",
    "from datetime import timedelta\n",
    "from collections import Counter\n",
    "from scipy.stats import gaussian_kde\n",
    "import glob, os\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _rescale_data(X, Y, sample_weight):\n",
    "    \"\"\"Rescale data so as to support sample_weight\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    sqrtW = np.sqrt(sample_weight)\n",
    "    \n",
    "    newX = X * sqrtW.reshape(-1,1)\n",
    "    newY = Y * sqrtW.reshape(-1,1)\n",
    "    \n",
    "    return newX, newY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addIntercept(X):\n",
    "    t = X.shape[0]\n",
    "    X_with_bias = np.hstack((np.ones((t, 1)), X))\n",
    "    return X_with_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    \"\"\"\n",
    "    A generic supervised model for data with input and output.\n",
    "    BaseModel does nothing, but lays out the methods expected of any subclass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fam, solver, fit_intercept = True, penalty = None, reg = 0, l1_ratio = 0, tol = 1e-4, max_iter = 100):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        Parameters\n",
    "        ----------\n",
    "        fam: family of the GLM, LM or MNL\n",
    "        solver: family specific solver\n",
    "        penalty: penalty to regularize the model\n",
    "        reg: regularization strenth\n",
    "        l1_ratio: if elastic net, the l1 reg ratio\n",
    "        tol: tol in the optimization procedure\n",
    "        max_iter: max_iter in the optimization procedure\n",
    "        -------\n",
    "        \"\"\"\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.penalty = penalty\n",
    "        self.reg = reg\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.fam = fam\n",
    "        self.solver = solver\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        \n",
    "\n",
    "    def fit(self, X, Y, sample_weight = None):\n",
    "        \"\"\"\n",
    "        fit the weighted model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Y : response matrix\n",
    "        sample_weight: sample weight vector\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict the Y value based on the model\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Returns\n",
    "        -------\n",
    "        predicted value\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "\n",
    "    def probability(self, X, Y):\n",
    "        \"\"\"\n",
    "        Given a set of X and Y, calculate the probability of\n",
    "        observing Y value\n",
    "        \"\"\"\n",
    "        logP = self.log_probability(X, Y)\n",
    "        if logP is not None:\n",
    "            return np.exp(self.log_probability(X, Y))\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def log_probability(self, X, Y):\n",
    "        \"\"\"\n",
    "        Given a set of X and Y, calculate the log probability of\n",
    "        observing each of Y value given each X value\n",
    "        \n",
    "        should return a vector\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def estimate_dispersion(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def estimate_sd(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def estimate_loglikelihood(self):\n",
    "        raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GLM(BaseModel):\n",
    "    \"\"\"\n",
    "    A Generalized linear model for data with input and output.\n",
    "    \"\"\"\n",
    "    def __init__(self, fam, solver = 'pinv', fit_intercept = True, penalty = None, reg = 0, l1_ratio = 0, tol = 1e-4, max_iter = 100):\n",
    "        super(GLM, self).__init__(fam = fam, solver = solver, fit_intercept = fit_intercept, penalty = penalty, \n",
    "                                 reg = reg, l1_ratio = l1_ratio, tol = tol, max_iter = max_iter)\n",
    "    def fit(self, X, Y, sample_weight = None):\n",
    "        \"\"\"\n",
    "        fit the weighted model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Y : response matrix\n",
    "        sample_weight: sample weight vector\n",
    "\n",
    "        \"\"\"\n",
    "         # family is the glm family with link, the family is the same as in the statsmodel\n",
    "        \n",
    "\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones((X.shape[0],))\n",
    "        assert X.shape[0] == sample_weight.shape[0]\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        assert Y.ndim == 1 or Y.shape[1] == 1\n",
    "        Y = Y.reshape(-1,)\n",
    "\n",
    "        sum_w = np.sum(sample_weight)\n",
    "        assert sum_w > 0\n",
    "        \n",
    "        \n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        if self.fit_intercept:\n",
    "            X = addIntercept(X)   \n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_targets = 1\n",
    "\n",
    "\n",
    "        # start fitting using irls\n",
    "        mu = self.fam.starting_mu(Y)\n",
    "        lin_pred = self.fam.predict(mu)\n",
    "        dev = self.fam.deviance_weighted(Y, mu, sample_weight)\n",
    "\n",
    "        if np.isnan(dev):\n",
    "            raise ValueError(\"The first guess on the deviance function \"\n",
    "                             \"returned a nan.  This could be a boundary \"\n",
    "                             \" problem and should be reported.\")\n",
    "\n",
    "        # This special case is used to get the likelihood for a specific\n",
    "        # params vector.\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "#             print sample_weight.shape\n",
    "#             print self.fam.weights(mu)\n",
    "            weights = sample_weight * self.fam.weights(mu)\n",
    "            wlsendog = lin_pred + self.fam.link.deriv(mu) * (Y-mu)\n",
    "            if self.penalty is None:\n",
    "                wls_results = lim.WLS(wlsendog, X, weights).fit(method = self.solver)\n",
    "\n",
    "            if self.penalty == 'elasticnet':\n",
    "                wls_results = lim.WLS(wlsendog, X, weights).fit_regularized(alpha = self.reg, L1_wt = self.l1_ratio)\n",
    "\n",
    "\n",
    "            lin_pred = np.dot(X, wls_results.params)\n",
    "            mu = self.fam.fitted(lin_pred)\n",
    "\n",
    "            if Y.squeeze().ndim == 1 and np.allclose(mu - Y, 0):\n",
    "                msg = \"Perfect separation detected, results not available\"\n",
    "                raise Error(msg)\n",
    "\n",
    "            dev_new = self.fam.deviance_weighted(Y, mu, sample_weight)\n",
    "            converged = np.fabs(dev - dev_new) <= self.tol\n",
    "            dev = dev_new\n",
    "            if converged:\n",
    "                break\n",
    "        \n",
    "        self.converged = converged\n",
    "        self.coef = wls_results.params\n",
    "        self.dispersion = self.estimate_dispersion(X, Y, mu, sample_weight)\n",
    "        self.sd = self.estimate_sd(X, Y, mu, sample_weight, weights)\n",
    "        self.ll = self.estimate_loglikelihood(Y, mu, sample_weight)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict the Y value based on the model\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Returns\n",
    "        -------\n",
    "        predicted value\n",
    "        \"\"\"\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        if self.fit_intercept:\n",
    "            X = addIntercept(X)\n",
    "        lin_pred = np.dot(X, self.coef)\n",
    "        mu = self.fam.fitted(lin_pred)\n",
    "        return mu\n",
    "\n",
    "    \n",
    "    def log_probability(self, X, Y):\n",
    "        \"\"\"\n",
    "        Given a set of X and Y, calculate the probability of\n",
    "        observing Y value\n",
    "        \"\"\"\n",
    "\n",
    "        mu = self.predict(X)\n",
    "#         print mu.shape\n",
    "#         print X.shape\n",
    "#         print Y.shape\n",
    "        return self.fam.log_probability(Y.reshape(-1,), mu, scale=self.dispersion)\n",
    "    \n",
    "    \n",
    "    def estimate_dispersion(self, X, Y, mu, w):\n",
    "        if isinstance(self.fam, (family.Binomial, family.Poisson)):\n",
    "            return 1\n",
    "        else:\n",
    "            resid = (Y - mu)\n",
    "            return (resid ** 2 * w / self.fam.variance(mu)).sum()/ np.sum(w)\n",
    "    \n",
    "    def estimate_sd(self, X, Y, mu, w, weights):\n",
    "        if self.penalty is None and self.dispersion is not None:\n",
    "            newX, newY = _rescale_data(X, Y, weights)\n",
    "            wX, wY = _rescale_data(X, Y, w * weights)\n",
    "            if X.shape[1] == 1:\n",
    "                try:\n",
    "                    cov = 1 / np.dot(newX.T, newX)    \n",
    "                    temp = np.dot(wX.T, wX) \n",
    "                    sd = (np.sqrt(cov ** 2 * temp)  * np.sqrt(self.dispersion)).reshape(-1,)\n",
    "\n",
    "                except:\n",
    "                    sd = None         \n",
    "            else:\n",
    "                try:\n",
    "                    cov = np.linalg.inv(np.dot(newX.T, newX))\n",
    "                    temp = np.dot(cov, wX.T)\n",
    "                    sd = np.sqrt(np.diag(np.dot(temp,temp.T))) * np.sqrt(self.dispersion)\n",
    "                except:\n",
    "                    sd = None\n",
    "        else:\n",
    "            sd = None\n",
    "        return sd\n",
    "\n",
    "    \n",
    "    def estimate_loglikelihood(self, Y, mu, w):\n",
    "        if self.dispersion is None:\n",
    "            return None\n",
    "        else:\n",
    "            return self.fam.loglike_weighted(Y, mu, w, scale=self.dispersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LM(BaseModel):\n",
    "    \"\"\"\n",
    "    A Generalized linear model for data with input and output.\n",
    "    \"\"\"\n",
    "    def __init__(self, solver = 'svd', fit_intercept = True, penalty = None, reg = 0, l1_ratio = 0, tol = 1e-4, max_iter = 100):\n",
    "        super(LM, self).__init__(fam = 'LM', solver = solver, fit_intercept = fit_intercept, penalty = penalty, \n",
    "                                 reg = reg, l1_ratio = l1_ratio, tol = tol, max_iter = max_iter)\n",
    "        \n",
    "    def fit(self, X, Y, sample_weight = None):\n",
    "        \"\"\"\n",
    "        fit the weighted model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Y : response matrix\n",
    "        sample_weight: sample weight vector\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones((X.shape[0],))\n",
    "        assert X.shape[0] == sample_weight.shape[0]\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "\n",
    "        sum_w = np.sum(sample_weight)\n",
    "        assert sum_w > 0\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        if self.fit_intercept:\n",
    "            X = addIntercept(X)\n",
    "        if Y.ndim == 1:\n",
    "            Y = Y.reshape(-1,1)\n",
    "\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_targets = Y.shape[1]\n",
    "        \n",
    "        newX, newY = _rescale_data(X, Y, sample_weight)\n",
    "\n",
    "\n",
    "        if self.penalty is None:\n",
    "            model = linear_model.LinearRegression(fit_intercept=False)\n",
    "        if self.penalty == 'l1':\n",
    "            model = linear_model.Lasso(fit_intercept=False, alpha = self.reg, tol = self.tol, max_iter = self.max_iter)\n",
    "        if self.penalty == 'l2':\n",
    "            model = linear_model.Ridge(fit_intercept=False, alpha = self.reg, tol = self.tol, \n",
    "                                       max_iter = self.max_iter, solver = self.solver)\n",
    "        if self.penalty == 'elasticnet':\n",
    "            model = linear_model.ElasticNet(fit_intercept=False, alpha = self.reg, l1_ratio = self.l1_ratio, \n",
    "                                            tol = self.tol, max_iter = self.max_iter)\n",
    "\n",
    "        model.fit(newX, newY)\n",
    "        self.coef = model.coef_.T\n",
    "        if Y.shape[1] == 1:\n",
    "            self.coef = self.coef.reshape(-1,)\n",
    "        if self.penalty is not None:\n",
    "            self.converged = model.n_iter_ < self.max_iter\n",
    "        else:\n",
    "            self.converged = None\n",
    "#         print X.shape\n",
    "        self.dispersion = self.estimate_dispersion(X, Y, sample_weight)\n",
    "        self.sd = self.estimate_sd(X, Y, sample_weight)\n",
    "        self.ll = self.estimate_loglikelihood(sample_weight)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict the Y value based on the model\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Returns\n",
    "        -------\n",
    "        predicted value\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        if self.fit_intercept:\n",
    "            X = addIntercept(X)\n",
    "        mu = np.dot(X, self.coef)\n",
    "        return mu\n",
    "\n",
    "    def log_probability(self, X, Y):\n",
    "        \"\"\"\n",
    "        Given a set of X and Y, calculate the probability of\n",
    "        observing Y value\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        if self.fit_intercept:\n",
    "            X = addIntercept(X)\n",
    "        if Y.ndim == 1:\n",
    "            Y = Y.reshape(-1,1)\n",
    "#         print self.n_features\n",
    "#         print X.shape\n",
    "#         assert X.shape[1] == self.n_features\n",
    "#         assert Y.shape[1] == self.n_targets\n",
    "        \n",
    "        pred = np.dot(X, self.coef)\n",
    "        if pred.ndim == 1:\n",
    "            pred = pred.reshape(-1,1)\n",
    "        \n",
    "        if Y.shape[1] == 1:\n",
    "            if self.dispersion > 0:\n",
    "                logP = (Y * pred - pred**2/2)/self.dispersion - Y**2/(2 * self.dispersion) - .5*np.log(2 * np.pi * self.dispersion)\n",
    "                logP = logP.reshape(-1,)\n",
    "            else:\n",
    "#                 return None\n",
    "                logP = np.zeros((Y.shape[0],))\n",
    "                logP[Y.reshape(-1,)!=pred.reshape(-1,)] = -np.Infinity\n",
    "                logP = logP.reshape(-1,)\n",
    "        else:\n",
    "            if np.linalg.det(self.dispersion) > 0:\n",
    "                logP = -1/2*((Y.shape[1] * np.log(2 * np.pi) + np.log(np.linalg.det(self.dispersion))) +\n",
    "                            np.diag(np.dot(np.dot(Y-pred, np.linalg.inv(self.dispersion)), (Y-pred).T)))\n",
    "                logP = logP.reshape(-1,)\n",
    "            else:\n",
    "                if (np.diag(self.dispersion) > 0).all():\n",
    "                    new_dispersion = np.diag(np.diag(self.dispersion))\n",
    "                    logP = -1/2*((Y.shape[1] * np.log(2 * np.pi) + np.log(np.linalg.det(self.dispersion))) + \n",
    "                                 np.diag(np.dot(np.dot(Y-pred, np.linalg.inv(new_dispersion)), (Y-pred).T)))\n",
    "                    logP = logP.reshape(-1,)\n",
    "\n",
    "                else:\n",
    "#                     return None\n",
    "                    logP = np.zeros((Y.shape[0],))\n",
    "                    logP[np.linalg.norm(Y-pred, axis = 1)!=0] = -np.Infinity\n",
    "                    logP = logP.reshape(-1,)\n",
    "        return logP\n",
    "    \n",
    "    \n",
    "    def estimate_dispersion(self, X, Y, sample_weight):\n",
    "        newX, newY = _rescale_data(X, Y, sample_weight)\n",
    "        newPred = np.dot(newX, self.coef)\n",
    "        if newPred.ndim == 1:\n",
    "            newPred = newPred.reshape(-1,1)\n",
    "        wresid = newY - newPred\n",
    "        ssr = np.dot(wresid.T, wresid)\n",
    "        sigma2 = ssr / np.sum(sample_weight)\n",
    "        if sigma2.shape == (1,1):\n",
    "            sigma2 = sigma2[0,0]\n",
    "        return sigma2\n",
    "        \n",
    "        \n",
    "    \n",
    "    def estimate_sd(self, X, Y, sample_weight):\n",
    "        newX, newY = _rescale_data(X, Y, sample_weight)\n",
    "        if self.penalty is None:\n",
    "            wX, wY = _rescale_data(X, Y, sample_weight ** 2)\n",
    "            if newX.shape[1] == 1:\n",
    "                try:\n",
    "                    cov = 1 / np.dot(newX.T, newX)    \n",
    "                    temp = np.dot(wX.T, wX) \n",
    "                    if newY.shape[1] == 1:\n",
    "                        sd = np.sqrt(cov ** 2 * temp * self.dispersion).reshape(-1,)\n",
    "                    else:\n",
    "                        sd = np.sqrt(cov ** 2 * temp * np.diag(self.dispersion))\n",
    "                except:\n",
    "                    sd = None         \n",
    "            else:\n",
    "                try:\n",
    "                    cov = np.linalg.inv(np.dot(newX.T, newX))\n",
    "                    temp = np.dot(cov, wX.T)\n",
    "                    if newY.shape[1] == 1:\n",
    "                        sd = np.sqrt(np.diag(np.dot(temp,temp.T)) * self.dispersion).reshape(-1,)\n",
    "                    else:\n",
    "                        sd = np.sqrt(np.outer(np.diag(np.dot(temp,temp.T)), np.diag(self.dispersion)))\n",
    "                except:\n",
    "                    sd = None\n",
    "        else:\n",
    "            sd = None\n",
    "        \n",
    "        return sd\n",
    "\n",
    "    \n",
    "    def estimate_loglikelihood(self, sample_weight):\n",
    "        q = self.n_targets\n",
    "        sum_w = np.sum(sample_weight)\n",
    "        if q == 1:\n",
    "            if self.dispersion > 0:\n",
    "                ll = - q * sum_w / 2 * np.log(2 * math.pi) - sum_w / 2 * np.log(self.dispersion) - q * sum_w / 2\n",
    "            else:\n",
    "                ll = None\n",
    "        else:\n",
    "            if np.linalg.det(self.dispersion) > 0:\n",
    "                ll = - q * sum_w / 2 * np.log(2 * math.pi) - sum_w / 2 * np.log(np.linalg.det(self.dispersion)) - q * sum_w / 2\n",
    "            else:\n",
    "                if (np.diag(self.dispersion) > 0).all():\n",
    "                    ll = - q * sum_w / 2 * np.log(2 * math.pi) - np.sum(sum_w / 2 * np.log(np.diag(self.dispersion))) - q * sum_w / 2\n",
    "                else:\n",
    "                    ll = None\n",
    "        return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNL(BaseModel):\n",
    "    \"\"\"\n",
    "    A MNL for data with input and output.\n",
    "    \"\"\"\n",
    "        \n",
    "    def fit(self, X, Y, sample_weight = None):\n",
    "        \"\"\"\n",
    "        fit the weighted model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Y : response matrix\n",
    "        sample_weight: sample weight vector\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict_probability(self, X):\n",
    "        \"\"\"\n",
    "        predict the Y value based on the model\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Returns\n",
    "        -------\n",
    "        predicted value\n",
    "        \"\"\"\n",
    "        return np.exp(self.predict_log_probability(X))\n",
    "    \n",
    "    def predict_log_probability(self, X):\n",
    "        \"\"\"\n",
    "        predict the Y value based on the model\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Returns\n",
    "        -------\n",
    "        predicted value\n",
    "        \"\"\"\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        if self.fit_intercept:\n",
    "            X = addIntercept(X)\n",
    "#         print X\n",
    "        p = np.dot(X, self.coef)\n",
    "#         print p\n",
    "        if p.ndim == 1:\n",
    "            p = p.reshape(-1,1)\n",
    "        p -= logsumexp(p, axis = 1)[:, np.newaxis]\n",
    "        return p\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict the Y value based on the model\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Returns\n",
    "        -------\n",
    "        predicted value\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "        \n",
    "\n",
    "    def log_probability(self, X, Y):\n",
    "        \"\"\"\n",
    "        Given a set of X and Y, calculate the probability of\n",
    "        observing Y value\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        return NotImplementedError\n",
    "        \n",
    "        \n",
    "    \n",
    "    def estimate_sd(self, X, sample_weight):\n",
    "        if self.penalty == None:\n",
    "            o_normalized = np.dot(X, self.coef)\n",
    "            if o_normalized.ndim == 1:\n",
    "                o_normalized = o_normalized.reshape(-1,1)\n",
    "            o_normalized -= logsumexp(o_normalized, axis = 1)[:, np.newaxis]\n",
    "            o_normalized = np.exp(o_normalized)\n",
    "            # calculate hessian\n",
    "            p = self.n_features\n",
    "            q = self.n_targets\n",
    "            h = np.zeros((p*(q-1), p*(q-1)))\n",
    "            for e in range(q-1):\n",
    "                for f in range(q-1):\n",
    "                    h[e*p: (e+1)*p, f*p: (f+1)*p] = -np.dot(np.dot(X.T, \n",
    "                                                                  np.diag(np.multiply(np.multiply(o_normalized[:, f+1], \n",
    "                                                                                                  (e==f) - o_normalized[:, e+1]), \n",
    "                                                                                     sample_weight))), X)\n",
    "            if np.sum(sample_weight) > 0:\n",
    "                h = h / np.sum(sample_weight) * X.shape[0]\n",
    "            if np.all(np.linalg.eigvals(-h) > 0):\n",
    "                sd = np.sqrt(np.diag(np.linalg.inv(-h))).reshape(p,q-1, order = 'F')\n",
    "                sd = np.hstack((np.zeros((p, 1)), sd))\n",
    "            else:\n",
    "                sd = None\n",
    "        else:\n",
    "            sd = None\n",
    "        return sd\n",
    "\n",
    "    \n",
    "    def estimate_loglikelihood(self, X, Y, sample_weight):\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNLD(MNL):\n",
    "    \"\"\"\n",
    "    A MNL for discrete data with input and output.\n",
    "    \"\"\"\n",
    "    def __init__(self, solver='newton-cg', fit_intercept = True, penalty = None, reg = 0, l1_ratio = 0, tol = 1e-4, max_iter = 100):\n",
    "        super(MNLD, self).__init__(fam = 'MNLD', solver = solver, fit_intercept = fit_intercept, penalty = penalty, \n",
    "                                 reg = reg, l1_ratio = l1_ratio, tol = tol, max_iter = max_iter)\n",
    "        \n",
    "    def fit(self, X, Y, sample_weight = None):\n",
    "        \"\"\"\n",
    "        fit the weighted model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Y : response matrix\n",
    "        sample_weight: sample weight vector\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones((X.shape[0],))\n",
    "        assert Y.ndim == 1 or Y.shape[1] == 1\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        assert X.shape[0] == sample_weight.shape[0]\n",
    "\n",
    "        if self.reg == 0 or (self.penalty is None):\n",
    "            penalty1 = 'l2'\n",
    "            c = 1e200\n",
    "        else:\n",
    "            penalty1 = self.penalty\n",
    "            c = 1/self.reg\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        if self.fit_intercept:\n",
    "            X = addIntercept(X)\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_targets = len(np.unique(Y))\n",
    "        if self.n_targets < 2:\n",
    "            raise ValueError('n_targets < 2')\n",
    "        \n",
    "        self.lb = LabelBinarizer().fit(Y)\n",
    "\n",
    "        model = linear_model.LogisticRegression(fit_intercept = False, penalty = penalty1, C = c,\n",
    "                                                multi_class = 'multinomial', solver = self.solver,\n",
    "                                                tol = self.tol, max_iter = self.max_iter)\n",
    "#         print X.shape\n",
    "#         print Y.shape\n",
    "        Y_fit = self.lb.transform(Y)\n",
    "        model.fit(X, Y, sample_weight = sample_weight)\n",
    "        w0 = model.coef_\n",
    "        if self.n_targets == 2:\n",
    "            w0 = np.vstack((np.zeros((1, self.n_features)), w0*2))\n",
    "#             w0 = np.vstack((w0,np.zeros((1, self.n_features))))\n",
    "#         print w0.shape\n",
    "#         print self.n_targets\n",
    "        w1 = w0.reshape(self.n_targets, -1)\n",
    "        w1 = w1.T - w1.T[:,0].reshape(-1,1)\n",
    "        self.coef = w1\n",
    "        self.converged = model.n_iter_ < self.max_iter\n",
    "        self.sd = self.estimate_sd(X, sample_weight)\n",
    "        self.ll = self.estimate_loglikelihood(X, Y, sample_weight)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict the Y value based on the model\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Returns\n",
    "        -------\n",
    "        predicted value\n",
    "        \"\"\"\n",
    "        index = np.argmax(self.predict_log_probability(X), axis = 1)\n",
    "        zero = np.zeros((X.shape[0], self.n_targets))\n",
    "        zero[np.arange(X.shape[0]), index] = 1\n",
    "        return self.lb.inverse_transform(zero)\n",
    "#         return index\n",
    "        \n",
    "\n",
    "    def log_probability(self, X, Y):\n",
    "        \"\"\"\n",
    "        Given a set of X and Y, calculate the probability of\n",
    "        observing Y value\n",
    "        \n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        assert Y.ndim == 1 or Y.shape[1] == 1\n",
    "#         if self.fit_intercept:\n",
    "#             assert X.shape[1] == self.n_features - 1\n",
    "#         else:\n",
    "#             assert X.shape[1] == self.n_features\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        \n",
    "        p = self.predict_log_probability(X)\n",
    "        Y_transformed = self.lb.transform(Y)\n",
    "        if Y_transformed.shape[1] == 1:\n",
    "            Y_aug = np.zeros((X.shape[0],2))\n",
    "            Y_aug[np.arange(X.shape[0]),Y_transformed.reshape(-1,)] = 1\n",
    "        else:\n",
    "            Y_aug = Y_transformed\n",
    "        logP = np.sum(p*Y_aug, axis = 1)\n",
    "        \n",
    "        return logP\n",
    "        \n",
    "        \n",
    "    \n",
    "    def estimate_loglikelihood(self, X, Y, sample_weight):\n",
    "        o_normalized_log = np.dot(X, self.coef)\n",
    "        if o_normalized_log.ndim == 1:\n",
    "            o_normalized_log = o_normalized_log.reshape(-1,1)\n",
    "        o_normalized_log -= logsumexp(o_normalized_log, axis = 1)[:, np.newaxis]\n",
    "        Y_aug = self.lb.transform(Y)\n",
    "        ll = (sample_weight[:, np.newaxis] * Y_aug * o_normalized_log).sum()\n",
    "        return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNLP(MNL):\n",
    "    \"\"\"\n",
    "    A MNL with probability response for data with input and output.\n",
    "    \"\"\"\n",
    "    def __init__(self, solver = 'newton-cg', fit_intercept = True, penalty = None, reg = 0, l1_ratio = 0, tol = 1e-4, max_iter = 100):\n",
    "        super(MNL, self).__init__(fam = 'MNLP', solver = solver, fit_intercept = fit_intercept, penalty = penalty, \n",
    "                                 reg = reg, l1_ratio = l1_ratio, tol = tol, max_iter = max_iter)\n",
    "        \n",
    "    def fit(self, X, Y, sample_weight = None):\n",
    "        \"\"\"\n",
    "        fit the weighted model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Y : response matrix\n",
    "        sample_weight: sample weight vector\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones((X.shape[0],))\n",
    "        \n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        assert X.shape[0] == sample_weight.shape[0]\n",
    "        \n",
    "        \n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        if self.fit_intercept:\n",
    "            X = addIntercept(X)\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_targets = Y.shape[1]\n",
    "        \n",
    "        if self.n_targets < 2:\n",
    "            raise ValueError('n_targets < 2')\n",
    "        \n",
    "        w0 = np.zeros((self.n_targets*self.n_features, ))\n",
    "\n",
    "        if self.solver == 'lbfgs':\n",
    "            func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]\n",
    "        else:\n",
    "            func = lambda x, *args: _multinomial_loss(x, *args)[0]\n",
    "            grad = lambda x, *args: _multinomial_loss_grad(x, *args)[1]\n",
    "            hess = _multinomial_grad_hess\n",
    "\n",
    "        if self.solver == 'lbfgs':\n",
    "            try:\n",
    "                w0, loss, info = optimize.fmin_l_bfgs_b(\n",
    "                    func, w0, fprime=None,\n",
    "                    args=(X, Y, self.reg, sample_weight),\n",
    "                    iprint=0, pgtol=self.tol, maxiter=self.max_iter)\n",
    "            except TypeError:\n",
    "                # old scipy doesn't have maxiter\n",
    "                w0, loss, info = optimize.fmin_l_bfgs_b(\n",
    "                    func, w0, fprime=None,\n",
    "                    args=(X, Y, self.reg, sample_weight),\n",
    "                    iprint=0, pgtol=self.tol)\n",
    "            if info[\"warnflag\"] == 1:\n",
    "                warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
    "                              \"of iterations.\")\n",
    "            try:\n",
    "                n_iter_i = info['nit'] - 1\n",
    "            except:\n",
    "                n_iter_i = info['funcalls'] - 1\n",
    "        else:\n",
    "            args = (X, Y, self.reg, sample_weight)\n",
    "            w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,\n",
    "                                             maxiter=self.max_iter, tol=self.tol)\n",
    "            \n",
    "\n",
    "    \n",
    "  \n",
    "        w1 = w0.reshape(self.n_targets, -1)\n",
    "        w1 = w1.T - w1.T[:,0].reshape(-1,1)\n",
    "        self.coef = w1\n",
    "        self.converged = n_iter_i < self.max_iter\n",
    "        self.sd = self.estimate_sd(X, sample_weight)\n",
    "        self.ll = self.estimate_loglikelihood(X, Y, sample_weight)\n",
    "\n",
    " \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict the Y value based on the model\n",
    "        ----------\n",
    "        X : design matrix\n",
    "        Returns\n",
    "        -------\n",
    "        predicted value\n",
    "        \"\"\"\n",
    "        index = np.argmax(self.predict_log_probability(X), axis = 1)\n",
    "        return index\n",
    "        \n",
    "\n",
    "    def log_probability(self, X, Y):\n",
    "        \"\"\"\n",
    "        Given a set of X and Y, calculate the probability of\n",
    "        observing Y value\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        assert Y.ndim == 2\n",
    "            \n",
    "#         if self.fit_intercept:\n",
    "#             assert X.shape[1] == self.n_features - 1\n",
    "#         else:\n",
    "#             assert X.shape[1] == self.n_features\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        \n",
    "        p = self.predict_log_probability(X)\n",
    "        logP = np.sum(p*Y, axis = 1)\n",
    "        \n",
    "        return logP\n",
    "        \n",
    "        \n",
    "    \n",
    "    def estimate_loglikelihood(self, X, Y, sample_weight):\n",
    "        o_normalized_log = np.dot(X, self.coef)\n",
    "        if o_normalized_log.ndim == 1:\n",
    "            o_normalized_log = o_normalized_log.reshape(-1,1)\n",
    "        o_normalized_log -= logsumexp(o_normalized_log, axis = 1)[:, np.newaxis]\n",
    "        ll = (sample_weight[:, np.newaxis] * Y * o_normalized_log).sum()\n",
    "        return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _multinomial_loss(w, X, Y, alpha, sample_weight):\n",
    "    \"\"\"Computes multinomial loss and class probabilities.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_classes * n_features,) or\n",
    "        (n_classes * (n_features + 1),)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    Y : ndarray, shape (n_samples, n_classes)\n",
    "        Transformed labels according to the output of LabelBinarizer.\n",
    "    alpha : float\n",
    "        Regularization parameter. alpha is equal to 1 / C.\n",
    "    sample_weight : array-like, shape (n_samples,) optional\n",
    "        Array of weights that are assigned to individual samples.\n",
    "        If not provided, then each sample is given unit weight.\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Multinomial loss.\n",
    "    p : ndarray, shape (n_samples, n_classes)\n",
    "        Estimated class probabilities.\n",
    "    w : ndarray, shape (n_classes, n_features)\n",
    "        Reshaped param vector excluding intercept terms.\n",
    "    Reference\n",
    "    ---------\n",
    "    Bishop, C. M. (2006). Pattern recognition and machine learning.\n",
    "    Springer. (Chapter 4.3.4)\n",
    "    \"\"\"\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    w = w.reshape(n_classes, -1)\n",
    "    sample_weight = sample_weight[:, np.newaxis]\n",
    "\n",
    "    p = np.dot(X, w.T)\n",
    "    p -= logsumexp(p, axis = 1)[:, np.newaxis]\n",
    "    loss = -(sample_weight * Y * p).sum()\n",
    "    loss += 0.5 * alpha * np.sum(w * w)\n",
    "    p = np.exp(p, p)\n",
    "    return loss, p, w\n",
    "def _multinomial_loss_grad(w, X, Y, alpha, sample_weight):\n",
    "    \"\"\"Computes the multinomial loss, gradient and class probabilities.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_classes * n_features,)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    Y : ndarray, shape (n_samples, n_classes)\n",
    "        Transformed labels according to the output of LabelBinarizer.\n",
    "    alpha : float\n",
    "        Regularization parameter. alpha is equal to 1 / C.\n",
    "    sample_weight : array-like, shape (n_samples,) optional\n",
    "        Array of weights that are assigned to individual samples.\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Multinomial loss.\n",
    "    grad : ndarray, shape (n_classes * n_features,) or\n",
    "        (n_classes * (n_features + 1),)\n",
    "        Ravelled gradient of the multinomial loss.\n",
    "    p : ndarray, shape (n_samples, n_classes)\n",
    "        Estimated class probabilities\n",
    "    Reference\n",
    "    ---------\n",
    "    Bishop, C. M. (2006). Pattern recognition and machine learning.\n",
    "    Springer. (Chapter 4.3.4)\n",
    "    \"\"\"\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    grad = np.zeros((n_classes, n_features))\n",
    "    loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)\n",
    "    sample_weight = sample_weight[:, np.newaxis]\n",
    "    diff = sample_weight * (p - Y)\n",
    "    grad[:, :n_features] = np.dot(diff.T, X)\n",
    "    grad[:, :n_features] += alpha * w\n",
    "    return loss, grad.ravel(), p\n",
    "def _multinomial_grad_hess(w, X, Y, alpha, sample_weight):\n",
    "    \"\"\"\n",
    "    Computes the gradient and the Hessian, in the case of a multinomial loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_classes * n_features,)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    Y : ndarray, shape (n_samples, n_classes)\n",
    "        Transformed labels according to the output of LabelBinarizer.\n",
    "    alpha : float\n",
    "        Regularization parameter. alpha is equal to 1 / C.\n",
    "    sample_weight : array-like, shape (n_samples,) optional\n",
    "        Array of weights that are assigned to individual samples.\n",
    "    Returns\n",
    "    -------\n",
    "    grad : array, shape (n_classes * n_features,) or\n",
    "        (n_classes * (n_features + 1),)\n",
    "        Ravelled gradient of the multinomial loss.\n",
    "    hessp : callable\n",
    "        Function that takes in a vector input of shape (n_classes * n_features)\n",
    "        or (n_classes * (n_features + 1)) and returns matrix-vector product\n",
    "        with hessian.\n",
    "    References\n",
    "    ----------\n",
    "    Barak A. Pearlmutter (1993). Fast Exact Multiplication by the Hessian.\n",
    "        http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    n_classes = Y.shape[1]\n",
    "\n",
    "    # `loss` is unused. Refactoring to avoid computing it does not\n",
    "    # significantly speed up the computation and decreases readability\n",
    "    loss, grad, p = _multinomial_loss_grad(w, X, Y, alpha, sample_weight)\n",
    "    sample_weight = sample_weight[:, np.newaxis]\n",
    "\n",
    "    # Hessian-vector product derived by applying the R-operator on the gradient\n",
    "    # of the multinomial loss function.\n",
    "    def hessp(v):\n",
    "        v = v.reshape(n_classes, -1)\n",
    "        # r_yhat holds the result of applying the R-operator on the multinomial\n",
    "        # estimator.\n",
    "        r_yhat = np.dot(X, v.T)\n",
    "        r_yhat += (-p * r_yhat).sum(axis=1)[:, np.newaxis]\n",
    "        r_yhat *= p\n",
    "        r_yhat *= sample_weight\n",
    "        hessProd = np.zeros((n_classes, n_features))\n",
    "        hessProd[:, :n_features] = np.dot(r_yhat.T, X)\n",
    "        hessProd[:, :n_features] += v * alpha\n",
    "        return hessProd.ravel()\n",
    "\n",
    "    return grad, hessp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calLogAlpha(log_prob_initial, log_prob_transition, log_Ey):\n",
    "\n",
    "    ## return the alpha matrix which should be t * k\n",
    "    # initial should be k * 1\n",
    "    # transition prob shoud be (t - 1) * k * k\n",
    "    # emission prob should be t * k * q\n",
    "    # Ey should be t * k\n",
    "    assert log_prob_initial.ndim == 1\n",
    "    assert log_prob_transition.ndim == 3\n",
    "    assert log_Ey.ndim == 2\n",
    "    t = log_Ey.shape[0]\n",
    "    k = log_Ey.shape[1]\n",
    "    log_alpha = np.zeros((t, k))\n",
    "    log_alpha[0, :] = log_prob_initial + log_Ey[0,:]\n",
    "    for i in range(1, t):\n",
    "        log_alpha[i, :] = logsumexp(log_prob_transition[i-1,:,:].T + log_alpha[i-1,:], axis = 1) + log_Ey[i,:]\n",
    "    assert log_alpha.ndim == 2\n",
    "    return log_alpha\n",
    "\n",
    "def calLogBeta(log_prob_transition, log_Ey):\n",
    "\n",
    "    ## return the alpha matrix which should be t * k\n",
    "    # pi should be k * 1\n",
    "    # transition prob shoud be (t - 1) * k * k\n",
    "    # emission prob should be t * k * q\n",
    "    # y should be t * k\n",
    "    assert len(log_prob_transition.shape) == 3\n",
    "    assert len(log_Ey.shape) == 2\n",
    "    t = log_Ey.shape[0]\n",
    "    k = log_Ey.shape[1]\n",
    "    log_beta = np.zeros((t, k))\n",
    "    for i in range(t-2, -1, -1):\n",
    "        log_beta[i, :] = logsumexp(log_prob_transition[i, :, :] + (log_beta[i+1, :] + log_Ey[i+1,:]), axis = 1)\n",
    "    assert len(log_beta.shape) == 2\n",
    "    return log_beta\n",
    "\n",
    "def calLogLikelihood(log_alpha):\n",
    "    return logsumexp(log_alpha[-1,:])\n",
    "\n",
    "def calLogGamma(log_alpha, log_beta, ll):\n",
    "    return log_alpha + log_beta - ll\n",
    "\n",
    "def calGamma(log_alpha, log_beta, ll):\n",
    "    return np.exp(calLogGamma(log_alpha, log_beta, ll))\n",
    "\n",
    "def calLogEpsilon(log_prob_transition, log_Ey, log_alpha, log_beta, ll):\n",
    "    # epsilon should be t - 1 * k * k\n",
    "    k = log_Ey.shape[1]\n",
    "    return np.tile((log_Ey + log_beta)[1:,np.newaxis,:], [1,k,1]) + np.tile(log_alpha[:-1,:,np.newaxis], [1,1,k]) + log_prob_transition - ll\n",
    "\n",
    "def calEpsilon(log_prob_transition, log_Ey, log_alpha, log_beta, ll):\n",
    "    return np.exp(calLogEpsilon(log_prob_transition, log_Ey, log_alpha, log_beta, ll))\n",
    "\n",
    "def calHMM(log_prob_initial, log_prob_transition, log_Ey):\n",
    "\n",
    "    log_alpha = calLogAlpha(log_prob_initial, log_prob_transition, log_Ey)\n",
    "    log_beta = calLogBeta(log_prob_transition, log_Ey)\n",
    "    ll = calLogLikelihood(log_alpha)\n",
    "    log_gamma = calLogGamma(log_alpha, log_beta, ll)\n",
    "    log_epsilon = calLogEpsilon(log_prob_transition, log_Ey, log_alpha, log_beta, ll)\n",
    "    return log_gamma, log_epsilon, ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SupervisedHMM:\n",
    "    def __init__(self, num_states = 2, EM_tol = 1e-4, max_EM_iter = 100):\n",
    "        self.num_states = num_states\n",
    "        self.EM_tol = EM_tol\n",
    "        self.max_EM_iter = max_EM_iter\n",
    "        \n",
    "    def setModels(self, model_emissions, model_initial = MNLP(), model_transition = MNLP()):\n",
    "        # initial model and transition model must be MNLP\n",
    "        self.model_initial = model_initial\n",
    "        self.model_transition = [deepcopy(model_transition) for i in range(self.num_states)]\n",
    "        self.model_emissions = [deepcopy(model_emissions) for i in range(self.num_states)]\n",
    "        self.num_emissions = len(model_emissions)\n",
    "    \n",
    "    def setData(self, dfs):\n",
    "        self.num_seqs = len(dfs)\n",
    "        self.dfs = dfs\n",
    "        \n",
    "    \n",
    "    def setInputs(self, covariates_initial, covariates_transition, covariates_emissions):\n",
    "        # input should be a list inidicating the columns of the dataframe\n",
    "        # 1. need to verify whether we need add intercept\n",
    "        # 2. need to verify numpy array\n",
    "        \n",
    "        \n",
    "#         self.inp_initials = [np.array(df.ix[0,covariates_initial]).reshape(1,-1).astype('float64') for df in self.dfs]\n",
    "        self.inp_initials = [np.array(df[covariates_initial].iloc[0]).reshape(1,-1).astype('float64') for df in self.dfs]\n",
    "        self.inp_initials_all_users = np.vstack(self.inp_initials)\n",
    "        self.model_initial.coef = np.zeros((self.inp_initials_all_users.shape[1]+self.model_initial.fit_intercept,self.num_states))\n",
    "        self.model_initial.coef = np.random.rand(self.inp_initials_all_users.shape[1]+self.model_initial.fit_intercept,self.num_states)\n",
    "        \n",
    "#         self.inp_transitions = [np.array(df.ix[1:, covariates_transition]).astype('float64') for df in self.dfs]\n",
    "        self.inp_transitions = [np.array(df[covariates_transition].iloc[1:]).astype('float64') for df in self.dfs]\n",
    "        self.inp_transitions_all_users = np.vstack(self.inp_transitions)\n",
    "        \n",
    "        for st in range(self.num_states):\n",
    "            self.model_transition[st].coef = np.zeros((self.inp_transitions_all_users.shape[1]+self.model_transition[st].fit_intercept,self.num_states))\n",
    "            self.model_transition[st].coef = np.random.rand(self.inp_transitions_all_users.shape[1]+self.model_transition[st].fit_intercept,self.num_states)\n",
    "        self.inp_emissions = []\n",
    "        self.inp_emissions_all_users = []\n",
    "        for cov in covariates_emissions:\n",
    "            self.inp_emissions.append([np.array(df[cov]).astype('float64') for df in self.dfs])\n",
    "        for covs in self.inp_emissions:\n",
    "            self.inp_emissions_all_users.append(np.vstack(covs))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def setOutputs(self, responses_emissions):\n",
    "        # output should be a list inidicating the columns of the dataframe\n",
    "        self.out_emissions = []\n",
    "        self.out_emissions_all_users = []\n",
    "        for res in responses_emissions:\n",
    "            self.out_emissions.append([np.array(df[res]) for df in self.dfs])\n",
    "        for ress in self.out_emissions:\n",
    "            self.out_emissions_all_users.append(np.vstack(ress))\n",
    "        for i in range(self.num_states):\n",
    "            for j in range(self.num_emissions):\n",
    "                if isinstance(self.model_emissions[i][j], GLM):\n",
    "                    self.model_emissions[i][j].coef = np.zeros((self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept,))\n",
    "                    self.model_emissions[i][j].coef = np.random.rand(self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept,)\n",
    "                    self.model_emissions[i][j].dispersion = 1\n",
    "                if isinstance(self.model_emissions[i][j], LM):\n",
    "                    if len(responses_emissions[j]) == 1:\n",
    "                        self.model_emissions[i][j].coef = np.zeros((self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept,))\n",
    "                        self.model_emissions[i][j].coef = np.random.rand(self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept,)\n",
    "                        self.model_emissions[i][j].dispersion = 1\n",
    "                    else:\n",
    "                        self.model_emissions[i][j].coef = np.zeros((self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept, len(responses_emissions[j])))\n",
    "                        self.model_emissions[i][j].coef = np.random.rand(self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept, len(responses_emissions[j]))\n",
    "                        self.model_emissions[i][j].dispersion = np.eye(len(responses_emissions[j]))\n",
    "                if isinstance(self.model_emissions[i][j], MNLD):\n",
    "                    self.model_emissions[i][j].coef = np.zeros((self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept,np.unique(self.out_emissions_all_users[j]).shape[0]))\n",
    "                    self.model_emissions[i][j].coef = np.random.rand(self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept,np.unique(self.out_emissions_all_users[j]).shape[0])\n",
    "                    self.model_emissions[i][j].lb = LabelBinarizer().fit(self.out_emissions_all_users[j])\n",
    "#                     self.model_emissions[i][j].n_targets = len(np.unique(self.out_emissions_all_users[j]))\n",
    "                if isinstance(self.model_emissions[i][j], MNLP):\n",
    "                    self.model_emissions[i][j].coef = np.zeros((self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept,len(responses_emissions[j])))\n",
    "                    self.model_emissions[i][j].coef = np.random.rand(self.inp_emissions_all_users[j].shape[1]+self.model_emissions[i][j].fit_intercept,len(responses_emissions[j]))\n",
    "    def EStep(self):\n",
    "        self.log_gammas = []\n",
    "        self.log_epsilons = []\n",
    "        self.lls = []\n",
    "        \n",
    "        for seq in range(self.num_seqs):\n",
    "            n_records = self.dfs[seq].shape[0]\n",
    "            log_prob_initial = self.model_initial.predict_log_probability(self.inp_initials[seq]).reshape(self.num_states,)\n",
    "            assert log_prob_initial.shape == (self.num_states,)\n",
    "            # t - 1 * k * k \n",
    "            log_prob_transition = np.zeros((n_records - 1, self.num_states, self.num_states))\n",
    "            for st in range(self.num_states):\n",
    "                 log_prob_transition[:,st,:] = self.model_transition[st].predict_log_probability(self.inp_transitions[seq]) \n",
    "#             log_prob_transition = np.vstack([model.predict_log_probability(self.inp_transitions[seq]) for model in self.model_transition]).reshape(self.num_states, n_records-1, self.num_states)\n",
    "#             log_prob_transition = np.swapaxes(log_prob_transition,0,1)\n",
    "            assert log_prob_transition.shape == (n_records-1,self.num_states,self.num_states)\n",
    "            \n",
    "            log_Ey = np.zeros((n_records,self.num_states))\n",
    "            for emis in range(self.num_emissions):\n",
    "                model_collection = [models[emis] for models in self.model_emissions]\n",
    "                log_Ey += np.vstack([model.log_probability(self.inp_emissions[emis][seq],\n",
    "                                                           self.out_emissions[emis][seq]) for model in model_collection]).T\n",
    "\n",
    "            \n",
    "            log_gamma, log_epsilon, ll = calHMM(log_prob_initial, log_prob_transition, log_Ey)\n",
    "            self.log_gammas.append(log_gamma)\n",
    "            self.log_epsilons.append(log_epsilon)\n",
    "            self.lls.append(ll)\n",
    "            self.ll = sum(self.lls)\n",
    "\n",
    "        \n",
    "    def MStep(self):\n",
    "        # optimize initial model\n",
    "        X = self.inp_initials_all_users\n",
    "        Y = np.exp(np.vstack([lg[0,:].reshape(1,-1) for lg in self.log_gammas]))\n",
    "        logY = np.vstack([lg[0,:].reshape(1,-1) for lg in self.log_gammas])\n",
    "        self.model_initial.fit(X, Y)\n",
    "        \n",
    "        # optimize transition models\n",
    "        X = self.inp_transitions_all_users\n",
    "        for st in range(self.num_states):\n",
    "            Y = np.exp(np.vstack([eps[:,st,:] for eps in self.log_epsilons]))\n",
    "            logY = np.vstack([eps[:,st,:] for eps in self.log_epsilons])\n",
    "            self.model_transition[st].fit(X, Y)\n",
    "        \n",
    "        # optimize emission models\n",
    "        for emis in range(self.num_emissions):\n",
    "            X = self.inp_emissions_all_users[emis]\n",
    "            Y = self.out_emissions_all_users[emis]\n",
    "            for st in range(self.num_states):\n",
    "                sample_weight = np.exp(np.hstack([lg[:,st] for lg in self.log_gammas]))\n",
    "#                 if sum(sample_weight) ==0:\n",
    "#                     'sum weight is 0'\n",
    "#                     print np.hstack([lg[:,st] for lg in self.log_gammas])\n",
    "#                     print sample_weight\n",
    "#                 if len(sample_weight[sample_weight>0]==1):\n",
    "#                     print np.hstack([lg[:,st] for lg in self.log_gammas])\n",
    "#                     print sample_weight\n",
    "                self.model_emissions[st][emis].fit(X, Y, sample_weight = sample_weight)\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        # maybe we should do some filtering here to filter out users with two few points.\n",
    "        # this filter may be applied here or before we train.\n",
    "\n",
    "        # set initial gamma and epsilons (to be equal likely).\n",
    "        # this can be seen as the first E-Step.\n",
    "#         a = np.random.rand(1,self.num_states)\n",
    "#         a -= logsumexp(a, axis = 1)\n",
    "#         self.log_gammas = [np.tile(a, [df.shape[0],1]) for df in self.dfs]\n",
    "#         b = np.random.rand(self.num_states,self.num_states)\n",
    "#         b -= logsumexp(b)\n",
    "#         self.log_epsilons = [np.tile(b[np.newaxis,:,:], [df.shape[0]-1,1,1]) for i in range(self.num_seqs)]\n",
    "#         self.ll = [-np.inf for i in range(self.num_seqs)]\n",
    "        self.EStep()\n",
    "        \n",
    "        # start training\n",
    "        for it in range(self.max_EM_iter):\n",
    "#             print self.log_epsilons\n",
    "            prev_ll = self.ll\n",
    "            self.MStep()\n",
    "            self.EStep()\n",
    "#             print [self.model_transition[i].coef for i in range(self.num_states)]\n",
    "            print self.ll\n",
    "            if abs(self.ll-prev_ll) < self.EM_tol:\n",
    "                break\n",
    "\n",
    "        self.converged = it < self.max_EM_iter\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests of SupervisedHMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0        rt corr  Pacc prev\n",
      "0           1  6.456770  cor     0  inc\n",
      "1           2  5.602119  cor     0  cor\n",
      "2           3  6.253829  inc     0  cor\n",
      "3           4  5.451038  inc     0  inc\n",
      "4           5  5.872118  inc     0  inc\n"
     ]
    }
   ],
   "source": [
    "speed = pd.read_csv('speed.csv')\n",
    "print speed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SHMM = SupervisedHMM(num_states=2, max_EM_iter=1000, EM_tol=1e-4)\n",
    "SHMM.setData([speed])\n",
    "SHMM.setModels(model_emissions = [LM()], model_transition=MNLP(solver='lbfgs'))\n",
    "SHMM.setInputs(covariates_initial = [], covariates_transition = [], covariates_emissions = [[]])\n",
    "SHMM.setOutputs([['rt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-305.268138632\n",
      "-305.241632572\n",
      "-305.203683475\n",
      "-305.148388635\n",
      "-305.062209716\n",
      "-304.926214958\n",
      "-304.679969726\n",
      "-304.243782014\n",
      "-303.351824605\n",
      "-301.086020078\n",
      "-292.76685388\n",
      "-264.148200699\n",
      "-176.576142995\n",
      "-102.236217332\n",
      "-92.6159593218\n",
      "-92.4053029356\n",
      "-92.4058061755\n",
      "-92.4254256689\n",
      "-92.4442018922\n",
      "-92.4634562729\n",
      "-92.4780725354\n",
      "-92.4893997056\n",
      "-92.4980134819\n",
      "-92.5044792334\n",
      "-92.5092928614\n",
      "-92.5128575614\n",
      "-92.5154881072\n",
      "-92.5174246645\n",
      "-92.5188479607\n",
      "-92.5198928121\n",
      "-92.52065921\n",
      "-92.5212210296\n",
      "-92.5216327043\n",
      "-92.5219342667\n",
      "-92.5221551195\n",
      "-92.5223168374\n",
      "-92.5224352401\n",
      "-92.5225219218\n"
     ]
    }
   ],
   "source": [
    "SHMM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.91014227  0.08985773]]\n",
      "[[ 0.19993659  0.80006341]]\n"
     ]
    }
   ],
   "source": [
    "print np.exp(SHMM.model_transition[0].coef - logsumexp(SHMM.model_transition[0].coef))\n",
    "print np.exp(SHMM.model_transition[1].coef - logsumexp(SHMM.model_transition[1].coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.37957519]\n",
      "[ 5.5013966]\n"
     ]
    }
   ],
   "source": [
    "print SHMM.model_emissions[0][0].coef\n",
    "print SHMM.model_emissions[1][0].coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.247431398302\n",
      "0.181899886497\n"
     ]
    }
   ],
   "source": [
    "print np.sqrt(SHMM.model_emissions[0][0].dispersion)\n",
    "print np.sqrt(SHMM.model_emissions[1][0].dispersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SHMM = SupervisedHMM(num_states=2, max_EM_iter=1000, EM_tol=1e-4)\n",
    "SHMM.setData([speed])\n",
    "SHMM.setModels(model_emissions = [LM(), MNLD()], model_transition=MNLP(solver='lbfgs'))\n",
    "SHMM.setInputs(covariates_initial = [], covariates_transition = [], covariates_emissions = [[],[]])\n",
    "SHMM.setOutputs([['rt'],['corr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-550.594802353\n",
      "-522.181893614\n",
      "-415.305368632\n",
      "-325.412145012\n",
      "-308.284355193\n",
      "-306.177101108\n",
      "-305.638562576\n",
      "-305.304622034\n",
      "-305.08796374\n",
      "-304.947607219\n",
      "-304.861742505\n",
      "-304.8096299\n",
      "-304.777782838\n",
      "-304.758066331\n",
      "-304.745686357\n",
      "-304.737814326\n",
      "-304.732757882\n",
      "-304.729485271\n",
      "-304.727355648\n",
      "-304.725964559\n",
      "-304.725053534\n",
      "-304.724455863\n",
      "-304.724063306\n",
      "-304.72380527\n",
      "-304.723635571\n",
      "-304.723523928\n",
      "-304.723450464\n"
     ]
    }
   ],
   "source": [
    "SHMM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.90881076  0.09118924]]\n",
      "[[ 0.19666679  0.80333321]]\n"
     ]
    }
   ],
   "source": [
    "print np.exp(SHMM.model_transition[0].coef - logsumexp(SHMM.model_transition[0].coef))\n",
    "print np.exp(SHMM.model_transition[1].coef - logsumexp(SHMM.model_transition[1].coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.38719782]\n",
      "[[ 0.         -2.18081328]]\n",
      "[ 5.51213225]\n",
      "[[ 0.         -0.10319339]]\n"
     ]
    }
   ],
   "source": [
    "print SHMM.model_emissions[0][0].coef\n",
    "print SHMM.model_emissions[0][1].coef\n",
    "print SHMM.model_emissions[1][0].coef\n",
    "print SHMM.model_emissions[1][1].coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SHMM = SupervisedHMM(num_states=2, max_EM_iter=1000, EM_tol=1e-4)\n",
    "SHMM.setData([speed])\n",
    "SHMM.setModels(model_emissions = [GLM(fam = family.Poisson()), MNLD()], model_transition=MNLP(solver='lbfgs'))\n",
    "SHMM.setInputs(covariates_initial = [], covariates_transition = [], covariates_emissions = [[],[]])\n",
    "SHMM.setOutputs([['rt_int'],['corr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1045.82448828\n",
      "-1045.67760078\n",
      "-1045.51471704\n",
      "-1045.31491643\n",
      "-1045.08434346\n",
      "-1044.73126144\n",
      "-1044.36098677\n",
      "-1043.81688361\n",
      "-1043.08053845\n",
      "-1041.49735439\n",
      "-1040.63174451\n",
      "-1039.99501723\n",
      "-1039.47115581\n",
      "-1039.059351\n",
      "-1038.74665126\n",
      "-1038.54032249\n",
      "-1038.38460192\n",
      "-1038.24264232\n",
      "-1038.14918097\n",
      "-1038.11083831\n",
      "-1038.04860336\n",
      "-1038.01240061\n",
      "-1037.96226007\n",
      "-1037.98714766\n",
      "-1037.93685961\n",
      "-1037.91821975\n",
      "-1037.96019243\n",
      "-1037.91821069\n",
      "-1037.9066653\n",
      "-1037.95493233\n",
      "-1037.91493139\n",
      "-1037.9058443\n",
      "-1037.89171742\n",
      "-1037.94754713\n",
      "-1037.90054861\n",
      "-1037.95457722\n",
      "-1037.9163299\n",
      "-1037.90877089\n",
      "-1037.8958214\n",
      "-1037.95250054\n",
      "-1037.91640291\n",
      "-1037.90944047\n",
      "-1037.89674511\n",
      "-1037.95409169\n",
      "-1037.9176654\n",
      "-1037.91072522\n",
      "-1037.89806868\n",
      "-1037.95547164\n",
      "-1037.91884422\n",
      "-1037.91185136\n",
      "-1037.89919239\n",
      "-1037.95647345\n",
      "-1037.9197331\n",
      "-1037.91268237\n",
      "-1037.90001294\n",
      "-1037.8848457\n",
      "-1037.87145843\n",
      "-1037.85791209\n",
      "-1037.84526843\n",
      "-1037.83397607\n",
      "-1037.82136735\n",
      "-1037.80977921\n",
      "-1039.06196961\n",
      "-1038.00097937\n",
      "-1037.9645974\n",
      "-1037.95890496\n",
      "-1037.95772355\n",
      "-1037.95747774\n",
      "-1037.95745808\n"
     ]
    }
   ],
   "source": [
    "SHMM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SHMM = SupervisedHMM(num_states=2, max_EM_iter=1000, EM_tol=1e-4)\n",
    "SHMM.setData([speed])\n",
    "SHMM.setModels(model_emissions = [MNLD()], model_transition=MNLP(solver='lbfgs'))\n",
    "SHMM.setInputs(covariates_initial = [], covariates_transition = [], covariates_emissions = [[]])\n",
    "SHMM.setOutputs([['corr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-249.482696414\n",
      "-249.386640372\n",
      "-249.304510389\n",
      "-249.231574364\n",
      "-249.163047803\n",
      "-249.284318179\n",
      "-249.216815871\n",
      "-249.167376909\n",
      "-249.113444861\n",
      "-249.053512542\n",
      "-248.98579058\n",
      "-248.951704081\n",
      "-248.789461652\n",
      "-248.698963032\n",
      "-248.645159658\n",
      "-248.761429865\n",
      "-248.666886524\n",
      "-248.573716263\n",
      "-248.706256008\n",
      "-248.595057361\n",
      "-248.485303981\n",
      "-248.629712815\n",
      "-248.49481264\n",
      "-248.360955526\n",
      "-248.698180639\n",
      "-248.516311175\n",
      "-248.38210355\n",
      "-248.232589498\n",
      "-248.622188523\n",
      "-248.394103779\n",
      "-248.224011055\n",
      "-248.086149689\n",
      "-248.513823566\n",
      "-248.230425016\n",
      "-248.01294718\n",
      "-248.187676593\n",
      "-247.919357356\n",
      "-248.395084689\n",
      "-248.040833101\n",
      "-247.783863256\n",
      "-248.274701534\n",
      "-247.874776483\n",
      "-248.026826303\n",
      "-247.638481359\n",
      "-247.639504897\n",
      "-246.928824195\n",
      "-246.992958444\n",
      "-246.404221748\n",
      "-245.964591168\n",
      "-245.406675037\n",
      "-244.917094801\n",
      "-244.412491055\n",
      "-243.970201417\n",
      "-243.666390172\n",
      "-243.372924216\n",
      "-243.218614018\n",
      "-243.537259176\n",
      "-243.253589585\n",
      "-243.171074479\n",
      "-243.074012094\n",
      "-243.038816472\n",
      "-242.896093749\n",
      "-242.801420126\n",
      "-242.748344751\n",
      "-242.697400146\n",
      "-242.660617604\n",
      "-242.632050165\n",
      "-242.605805423\n",
      "-242.586423665\n",
      "-242.936563354\n",
      "-242.673620898\n",
      "-242.629919746\n",
      "-242.591430129\n",
      "-242.556415894\n",
      "-242.941858767\n",
      "-242.658698526\n",
      "-242.609634367\n",
      "-242.570973495\n",
      "-242.530604784\n",
      "-242.948348535\n",
      "-242.650474695\n",
      "-242.594112569\n",
      "-242.553010914\n",
      "-242.511821513\n",
      "-242.954631528\n",
      "-242.634546346\n",
      "-242.580277877\n",
      "-242.536961213\n",
      "-242.496263171\n",
      "-242.954312387\n",
      "-242.622026641\n",
      "-242.569482144\n",
      "-242.524128959\n",
      "-242.481293226\n",
      "-242.951605158\n",
      "-242.608817727\n",
      "-242.550907732\n",
      "-242.510426687\n",
      "-242.468070191\n",
      "-242.954731675\n",
      "-242.592948475\n",
      "-242.562273727\n",
      "-242.510590378\n",
      "-242.465917277\n",
      "-242.959172869\n",
      "-242.717882956\n",
      "-242.612163851\n",
      "-242.561509486\n",
      "-242.517447553\n",
      "-242.473146975\n",
      "-242.953172888\n",
      "-242.60183872\n",
      "-242.541338191\n",
      "-242.495504552\n",
      "-242.455753796\n",
      "-242.950769896\n",
      "-242.585730327\n",
      "-242.563737064\n",
      "-242.698374377\n",
      "-242.554044795\n",
      "-242.498666983\n",
      "-242.955864505\n",
      "-242.628107326\n",
      "-242.569570869\n",
      "-242.516470204\n",
      "-242.474230077\n",
      "-242.95374219\n",
      "-242.608320545\n",
      "-242.552603333\n",
      "-242.502798482\n",
      "-242.460961499\n",
      "-242.953585621\n",
      "-242.592949963\n",
      "-242.564125998\n",
      "-242.513027944\n",
      "-242.463601928\n",
      "-242.958191083\n",
      "-242.713462315\n",
      "-242.613483118\n",
      "-242.561064377\n",
      "-242.509596538\n",
      "-242.468198789\n",
      "-242.954757526\n",
      "-242.599366991\n",
      "-242.539768539\n",
      "-242.493966206\n",
      "-242.453059248\n",
      "-242.955308194\n",
      "-242.581884949\n",
      "-242.563450101\n",
      "-242.515081447\n",
      "-242.476386862\n",
      "-242.440300965\n",
      "-242.955044916\n",
      "-242.591507801\n",
      "-242.565754454\n",
      "-242.558007285\n",
      "-242.50816357\n",
      "-242.459598349\n",
      "-242.959138182\n",
      "-242.716336788\n",
      "-242.612357225\n",
      "-242.559644843\n",
      "-242.507395381\n",
      "-242.465703154\n",
      "-242.955282371\n",
      "-242.597192379\n",
      "-242.536765891\n",
      "-242.491013415\n",
      "-242.450689739\n",
      "-242.951687888\n",
      "-242.58218077\n",
      "-242.563628962\n",
      "-242.515028297\n",
      "-242.465640243\n",
      "-242.960872468\n",
      "-242.718168687\n",
      "-242.612657787\n",
      "-242.560314711\n",
      "-242.509593894\n",
      "-242.467318869\n",
      "-242.955293691\n",
      "-242.598032484\n",
      "-242.537939407\n",
      "-242.492018791\n",
      "-242.451966424\n",
      "-242.951395414\n",
      "-242.583160215\n",
      "-242.563625172\n",
      "-242.515039126\n",
      "-242.465682847\n",
      "-242.960705284\n",
      "-242.718095378\n",
      "-242.612620452\n",
      "-242.560501352\n",
      "-242.509778989\n",
      "-242.467522785\n",
      "-242.955334467\n",
      "-242.597968432\n",
      "-242.537892172\n",
      "-242.492414974\n",
      "-242.452221505\n",
      "-242.951414314\n",
      "-242.583233754\n",
      "-242.56364694\n",
      "-242.515095666\n",
      "-242.476477273\n",
      "-242.440411833\n",
      "-242.955021134\n",
      "-242.591485241\n",
      "-242.565748806\n",
      "-242.558009473\n",
      "-242.508184365\n",
      "-242.459631994\n",
      "-242.959136906\n",
      "-242.71633346\n",
      "-242.612354579\n",
      "-242.559647828\n",
      "-242.507403221\n",
      "-242.465713325\n",
      "-242.955288378\n",
      "-242.597175711\n",
      "-242.536747565\n",
      "-242.490993648\n",
      "-242.450668985\n",
      "-242.951702052\n",
      "-242.582132529\n",
      "-242.563635397\n",
      "-242.515045803\n",
      "-242.465637518\n",
      "-242.960882457\n",
      "-242.718186945\n",
      "-242.61264988\n",
      "-242.56030277\n",
      "-242.509580931\n",
      "-242.467305511\n",
      "-242.955300988\n",
      "-242.598006693\n",
      "-242.537906427\n",
      "-242.491980438\n",
      "-242.451925179\n",
      "-242.951412636\n",
      "-242.583099936\n",
      "-242.563630524\n",
      "-242.515054514\n",
      "-242.476387752\n",
      "-242.440307719\n",
      "-242.95502017\n",
      "-242.591480976\n",
      "-242.565736946\n",
      "-242.557995543\n",
      "-242.508158863\n",
      "-242.459593725\n",
      "-242.959129519\n",
      "-242.71631665\n",
      "-242.612368542\n",
      "-242.559658672\n",
      "-242.507408289\n",
      "-242.465715734\n",
      "-242.955270949\n",
      "-242.597231132\n",
      "-242.536814236\n",
      "-242.491069141\n",
      "-242.450749396\n",
      "-242.951660767\n",
      "-242.582275052\n",
      "-242.563619119\n",
      "-242.515001989\n",
      "-242.465218304\n",
      "-242.960841447\n",
      "-242.718329304\n",
      "-242.612568501\n",
      "-242.560243325\n",
      "-242.509476644\n",
      "-242.467187911\n",
      "-242.955340077\n",
      "-242.597861331\n",
      "-242.537713309\n",
      "-242.491752045\n",
      "-242.451678497\n",
      "-242.951502843\n",
      "-242.582781098\n",
      "-242.563655308\n",
      "-242.515124155\n",
      "-242.476711945\n",
      "-242.4404907\n",
      "-242.954997355\n",
      "-242.59142544\n",
      "-242.565705758\n",
      "-242.557979786\n",
      "-242.508172357\n",
      "-242.459620104\n",
      "-242.959114787\n",
      "-242.716282025\n",
      "-242.612383484\n",
      "-242.559683137\n",
      "-242.507436164\n",
      "-242.465745425\n",
      "-242.95525918\n",
      "-242.597274742\n",
      "-242.536871105\n",
      "-242.491136045\n",
      "-242.450821423\n",
      "-242.951632745\n",
      "-242.58237349\n",
      "-242.563610292\n",
      "-242.514978572\n",
      "-242.465237891\n",
      "-242.960822691\n",
      "-242.718295573\n",
      "-242.612581377\n",
      "-242.560265982\n",
      "-242.509503304\n",
      "-242.467216387\n",
      "-242.955330114\n",
      "-242.597899037\n",
      "-242.537762908\n",
      "-242.491810482\n",
      "-242.451741558\n",
      "-242.951479102\n",
      "-242.582864738\n",
      "-242.56364847\n",
      "-242.515105167\n",
      "-242.476728612\n",
      "-242.440517115\n",
      "-242.954984093\n",
      "-242.59141265\n",
      "-242.565699601\n",
      "-242.557977069\n",
      "-242.508175934\n",
      "-242.459626719\n",
      "-242.959112108\n",
      "-242.716275719\n",
      "-242.612385965\n",
      "-242.559584377\n",
      "-242.507375796\n",
      "-242.465690205\n",
      "-242.955276079\n",
      "-242.597201984\n",
      "-242.536776571\n",
      "-242.491024768\n",
      "-242.450701593\n",
      "-242.951678423\n",
      "-242.582212797\n",
      "-242.563624437\n",
      "-242.515015889\n",
      "-242.465641145\n",
      "-242.960865731\n",
      "-242.718156337\n",
      "-242.612663248\n",
      "-242.560287531\n",
      "-242.509579965\n",
      "-242.467306904\n",
      "-242.955294845\n",
      "-242.598023548\n",
      "-242.537927268\n",
      "-242.492004203\n",
      "-242.451950604\n",
      "-242.951400096\n",
      "-242.583143283\n",
      "-242.56362606\n",
      "-242.515041677\n",
      "-242.476393475\n",
      "-242.440318472\n",
      "-242.955012392\n",
      "-242.591473306\n",
      "-242.565732825\n",
      "-242.557993338\n",
      "-242.508159813\n",
      "-242.459595876\n",
      "-242.95912763\n",
      "-242.716312228\n",
      "-242.61237061\n",
      "-242.559661772\n",
      "-242.507411648\n",
      "-242.465719241\n",
      "-242.955269167\n",
      "-242.597237516\n",
      "-242.536822428\n",
      "-242.491078708\n",
      "-242.450759676\n",
      "-242.951656529\n",
      "-242.582289877\n",
      "-242.563617704\n",
      "-242.514998227\n",
      "-242.465220985\n",
      "-242.96083859\n",
      "-242.718324156\n",
      "-242.612570502\n",
      "-242.56024677\n",
      "-242.509480655\n",
      "-242.467192177\n",
      "-242.955338482\n",
      "-242.597867295\n",
      "-242.537721118\n",
      "-242.491761226\n",
      "-242.451688399\n",
      "-242.951499046\n",
      "-242.58279445\n",
      "-242.563654188\n",
      "-242.515121053\n",
      "-242.47671454\n",
      "-242.440494851\n",
      "-242.954995217\n",
      "-242.591423374\n",
      "-242.565704754\n",
      "-242.557979334\n",
      "-242.508172908\n",
      "-242.459621131\n",
      "-242.959114348\n",
      "-242.716280992\n",
      "-242.612383898\n",
      "-242.559683871\n",
      "-242.507437035\n",
      "-242.465746367\n",
      "-242.955258884\n",
      "-242.59727588\n",
      "-242.536872616\n",
      "-242.491137836\n",
      "-242.450823356\n",
      "-242.95163204\n",
      "-242.582375976\n",
      "-242.563610086\n",
      "-242.514978027\n",
      "-242.46523844\n",
      "-242.960822223\n",
      "-242.718294734\n",
      "-242.612581689\n",
      "-242.560266548\n",
      "-242.509503978\n",
      "-242.467217111\n",
      "-242.955329881\n",
      "-242.597899933\n",
      "-242.537764094\n",
      "-242.491811883\n",
      "-242.451743071\n",
      "-242.951478546\n",
      "-242.582866701\n",
      "-242.563648315\n",
      "-242.515104736\n",
      "-242.476729017\n",
      "-242.440517748\n",
      "-242.954983786\n",
      "-242.591412354\n",
      "-242.565699461\n",
      "-242.557977009\n",
      "-242.508176022\n",
      "-242.45962688\n",
      "-242.959112048\n",
      "-242.716275576\n",
      "-242.612386019\n",
      "-242.559584479\n",
      "-242.507375919\n",
      "-242.46569034\n",
      "-242.955276043\n",
      "-242.597202129\n",
      "-242.536776765\n",
      "-242.491024999\n",
      "-242.450701843\n",
      "-242.951678336\n",
      "-242.582213106\n",
      "-242.563624413\n",
      "-242.515015825\n",
      "-242.465641219\n",
      "-242.960865674\n",
      "-242.718156235\n",
      "-242.612663285\n",
      "-242.5602876\n",
      "-242.509580049\n",
      "-242.467306994\n",
      "-242.955294818\n",
      "-242.598023654\n",
      "-242.537927409\n",
      "-242.492004369\n",
      "-242.451950784\n",
      "-242.951400032\n",
      "-242.583143513\n",
      "-242.563626043\n",
      "-242.515041628\n",
      "-242.476393523\n",
      "-242.440318548\n",
      "-242.955012356\n",
      "-242.591473272\n",
      "-242.565732809\n",
      "-242.557993332\n",
      "-242.508159824\n",
      "-242.459595896\n",
      "-242.959127623\n",
      "-242.716312211\n",
      "-242.612370616\n",
      "-242.559661784\n",
      "-242.507411663\n",
      "-242.465719256\n",
      "-242.955269163\n",
      "-242.597237532\n",
      "-242.53682245\n",
      "-242.491078734\n",
      "-242.450759703\n",
      "-242.95165652\n",
      "-242.58228991\n",
      "-242.563617702\n",
      "-242.51499822\n",
      "-242.465220993\n",
      "-242.960838584\n",
      "-242.718324145\n",
      "-242.612570506\n",
      "-242.560246778\n",
      "-242.509480664\n",
      "-242.467192186\n",
      "-242.955338479\n",
      "-242.597867306\n",
      "-242.537721133\n",
      "-242.491761243\n",
      "-242.451688418\n",
      "-242.95149904\n",
      "-242.582794473\n",
      "-242.563654186\n",
      "-242.515121049\n",
      "-242.476714545\n",
      "-242.440494858\n",
      "-242.954995214\n",
      "-242.591423371\n",
      "-242.565704752\n",
      "-242.557979334\n",
      "-242.508172909\n",
      "-242.459621133\n",
      "-242.959114347\n",
      "-242.71628099\n",
      "-242.612383899\n",
      "-242.559683872\n",
      "-242.507437037\n",
      "-242.465746368\n",
      "-242.955258884\n",
      "-242.597275882\n",
      "-242.536872618\n",
      "-242.491137839\n",
      "-242.450823359\n",
      "-242.95163204\n",
      "-242.58237598\n",
      "-242.563610086\n",
      "-242.514978026\n",
      "-242.465238441\n",
      "-242.960822223\n",
      "-242.718294733\n",
      "-242.61258169\n",
      "-242.560266549\n",
      "-242.509503979\n",
      "-242.467217112\n",
      "-242.955329881\n",
      "-242.597899934\n",
      "-242.537764095\n",
      "-242.491811885\n",
      "-242.451743073\n",
      "-242.951478546\n",
      "-242.582866703\n",
      "-242.563648315\n",
      "-242.515104735\n",
      "-242.476729017\n",
      "-242.440517748\n",
      "-242.954983785\n",
      "-242.591412354\n",
      "-242.56569946\n",
      "-242.557977009\n",
      "-242.508176022\n",
      "-242.45962688\n",
      "-242.959112048\n",
      "-242.716275576\n",
      "-242.612386019\n",
      "-242.559584479\n",
      "-242.50737592\n",
      "-242.46569034\n",
      "-242.955276043\n",
      "-242.597202129\n",
      "-242.536776765\n",
      "-242.491025\n",
      "-242.450701843\n",
      "-242.951678335\n",
      "-242.582213106\n",
      "-242.563624413\n",
      "-242.515015825\n",
      "-242.465641219\n",
      "-242.960865674\n",
      "-242.718156234\n",
      "-242.612663285\n",
      "-242.5602876\n",
      "-242.509580049\n",
      "-242.467306994\n",
      "-242.955294818\n",
      "-242.598023654\n",
      "-242.537927409\n",
      "-242.49200437\n",
      "-242.451950784\n",
      "-242.951400032\n",
      "-242.583143513\n",
      "-242.563626043\n",
      "-242.515041628\n",
      "-242.476393523\n",
      "-242.440318548\n",
      "-242.955012356\n",
      "-242.591473272\n",
      "-242.565732809\n",
      "-242.557993332\n",
      "-242.508159824\n",
      "-242.459595896\n",
      "-242.959127623\n",
      "-242.716312211\n",
      "-242.612370616\n",
      "-242.559661784\n",
      "-242.507411663\n",
      "-242.465719257\n",
      "-242.955269163\n",
      "-242.597237532\n",
      "-242.53682245\n",
      "-242.491078734\n",
      "-242.450759703\n",
      "-242.95165652\n",
      "-242.58228991\n",
      "-242.563617702\n",
      "-242.51499822\n",
      "-242.465220993\n",
      "-242.960838584\n",
      "-242.718324145\n",
      "-242.612570506\n",
      "-242.560246778\n",
      "-242.509480664\n",
      "-242.467192186\n",
      "-242.955338479\n",
      "-242.597867306\n",
      "-242.537721133\n",
      "-242.491761243\n",
      "-242.451688418\n",
      "-242.95149904\n",
      "-242.582794473\n",
      "-242.563654186\n",
      "-242.515121049\n",
      "-242.476714545\n",
      "-242.440494858\n",
      "-242.954995214\n",
      "-242.591423371\n",
      "-242.565704752\n",
      "-242.557979334\n",
      "-242.508172909\n",
      "-242.459621133\n",
      "-242.959114347\n",
      "-242.71628099\n",
      "-242.612383899\n",
      "-242.559683872\n",
      "-242.507437037\n",
      "-242.465746368\n",
      "-242.955258884\n",
      "-242.597275882\n",
      "-242.536872618\n",
      "-242.491137839\n",
      "-242.450823359\n",
      "-242.95163204\n",
      "-242.58237598\n",
      "-242.563610086\n",
      "-242.514978026\n",
      "-242.465238441\n",
      "-242.960822223\n",
      "-242.718294733\n",
      "-242.61258169\n",
      "-242.560266549\n",
      "-242.509503979\n",
      "-242.467217112\n",
      "-242.955329881\n",
      "-242.597899934\n",
      "-242.537764095\n",
      "-242.491811885\n",
      "-242.451743073\n",
      "-242.951478546\n",
      "-242.582866703\n",
      "-242.563648315\n",
      "-242.515104735\n",
      "-242.476729017\n",
      "-242.440517748\n",
      "-242.954983785\n",
      "-242.591412354\n",
      "-242.56569946\n",
      "-242.557977009\n",
      "-242.508176022\n",
      "-242.45962688\n",
      "-242.959112048\n",
      "-242.716275576\n",
      "-242.612386019\n",
      "-242.559584479\n",
      "-242.50737592\n",
      "-242.46569034\n",
      "-242.955276043\n",
      "-242.597202129\n",
      "-242.536776765\n",
      "-242.491025\n",
      "-242.450701843\n",
      "-242.951678335\n",
      "-242.582213106\n",
      "-242.563624413\n",
      "-242.515015825\n",
      "-242.465641219\n",
      "-242.960865674\n",
      "-242.718156234\n",
      "-242.612663285\n",
      "-242.5602876\n",
      "-242.509580049\n",
      "-242.467306994\n",
      "-242.955294818\n",
      "-242.598023654\n",
      "-242.537927409\n",
      "-242.49200437\n",
      "-242.451950784\n",
      "-242.951400032\n",
      "-242.583143513\n",
      "-242.563626043\n",
      "-242.515041628\n",
      "-242.476393523\n",
      "-242.440318548\n",
      "-242.955012356\n",
      "-242.591473272\n",
      "-242.565732809\n",
      "-242.557993332\n",
      "-242.508159824\n",
      "-242.459595896\n",
      "-242.959127623\n",
      "-242.716312211\n",
      "-242.612370616\n",
      "-242.559661784\n",
      "-242.507411663\n",
      "-242.465719257\n",
      "-242.955269163\n",
      "-242.597237532\n",
      "-242.53682245\n",
      "-242.491078734\n",
      "-242.450759703\n",
      "-242.95165652\n",
      "-242.58228991\n",
      "-242.563617702\n",
      "-242.51499822\n",
      "-242.465220993\n",
      "-242.960838584\n",
      "-242.718324145\n",
      "-242.612570506\n",
      "-242.560246778\n",
      "-242.509480664\n",
      "-242.467192186\n",
      "-242.955338479\n",
      "-242.597867306\n",
      "-242.537721133\n",
      "-242.491761243\n",
      "-242.451688418\n",
      "-242.95149904\n",
      "-242.582794473\n",
      "-242.563654186\n",
      "-242.515121049\n",
      "-242.476714545\n",
      "-242.440494858\n",
      "-242.954995214\n",
      "-242.591423371\n",
      "-242.565704752\n",
      "-242.557979334\n",
      "-242.508172909\n",
      "-242.459621133\n",
      "-242.959114347\n",
      "-242.71628099\n",
      "-242.612383899\n",
      "-242.559683872\n",
      "-242.507437037\n",
      "-242.465746368\n",
      "-242.955258884\n",
      "-242.597275882\n",
      "-242.536872618\n",
      "-242.491137839\n",
      "-242.450823359\n",
      "-242.95163204\n",
      "-242.58237598\n",
      "-242.563610086\n",
      "-242.514978026\n",
      "-242.465238441\n",
      "-242.960822223\n",
      "-242.718294733\n",
      "-242.61258169\n",
      "-242.560266549\n",
      "-242.509503979\n",
      "-242.467217112\n",
      "-242.955329881\n",
      "-242.597899934\n",
      "-242.537764095\n",
      "-242.491811885\n",
      "-242.451743073\n",
      "-242.951478546\n",
      "-242.582866703\n",
      "-242.563648315\n",
      "-242.515104735\n",
      "-242.476729017\n",
      "-242.440517748\n",
      "-242.954983785\n",
      "-242.591412354\n",
      "-242.56569946\n",
      "-242.557977009\n",
      "-242.508176022\n",
      "-242.45962688\n",
      "-242.959112048\n",
      "-242.716275576\n",
      "-242.612386019\n",
      "-242.559584479\n",
      "-242.50737592\n",
      "-242.46569034\n",
      "-242.955276043\n",
      "-242.597202129\n",
      "-242.536776765\n",
      "-242.491025\n",
      "-242.450701843\n",
      "-242.951678335\n",
      "-242.582213106\n",
      "-242.563624413\n",
      "-242.515015825\n",
      "-242.465641219\n",
      "-242.960865674\n",
      "-242.718156234\n",
      "-242.612663285\n",
      "-242.5602876\n",
      "-242.509580049\n",
      "-242.467306994\n",
      "-242.955294818\n",
      "-242.598023654\n",
      "-242.537927409\n",
      "-242.49200437\n",
      "-242.451950784\n",
      "-242.951400032\n",
      "-242.583143513\n",
      "-242.563626043\n",
      "-242.515041628\n",
      "-242.476393523\n",
      "-242.440318548\n",
      "-242.955012356\n",
      "-242.591473272\n",
      "-242.565732809\n",
      "-242.557993332\n",
      "-242.508159824\n",
      "-242.459595896\n",
      "-242.959127623\n",
      "-242.716312211\n",
      "-242.612370616\n",
      "-242.559661784\n",
      "-242.507411663\n",
      "-242.465719257\n",
      "-242.955269163\n",
      "-242.597237532\n",
      "-242.53682245\n",
      "-242.491078734\n",
      "-242.450759703\n",
      "-242.95165652\n",
      "-242.58228991\n",
      "-242.563617702\n",
      "-242.51499822\n",
      "-242.465220993\n",
      "-242.960838584\n",
      "-242.718324145\n",
      "-242.612570506\n",
      "-242.560246778\n",
      "-242.509480664\n",
      "-242.467192186\n",
      "-242.955338479\n",
      "-242.597867306\n",
      "-242.537721133\n",
      "-242.491761243\n",
      "-242.451688418\n",
      "-242.95149904\n",
      "-242.582794473\n",
      "-242.563654186\n",
      "-242.515121049\n",
      "-242.476714545\n",
      "-242.440494858\n",
      "-242.954995214\n",
      "-242.591423371\n",
      "-242.565704752\n",
      "-242.557979334\n",
      "-242.508172909\n",
      "-242.459621133\n",
      "-242.959114347\n",
      "-242.71628099\n",
      "-242.612383899\n",
      "-242.559683872\n",
      "-242.507437037\n",
      "-242.465746368\n",
      "-242.955258884\n",
      "-242.597275882\n",
      "-242.536872618\n",
      "-242.491137839\n",
      "-242.450823359\n",
      "-242.95163204\n",
      "-242.58237598\n",
      "-242.563610086\n",
      "-242.514978026\n",
      "-242.465238441\n",
      "-242.960822223\n",
      "-242.718294733\n",
      "-242.61258169\n",
      "-242.560266549\n",
      "-242.509503979\n",
      "-242.467217112\n",
      "-242.955329881\n",
      "-242.597899934\n",
      "-242.537764095\n",
      "-242.491811885\n",
      "-242.451743073\n",
      "-242.951478546\n",
      "-242.582866703\n",
      "-242.563648315\n",
      "-242.515104735\n",
      "-242.476729017\n",
      "-242.440517748\n",
      "-242.954983785\n",
      "-242.591412354\n",
      "-242.56569946\n",
      "-242.557977009\n",
      "-242.508176022\n",
      "-242.45962688\n",
      "-242.959112048\n",
      "-242.716275576\n",
      "-242.612386019\n",
      "-242.559584479\n",
      "-242.50737592\n",
      "-242.46569034\n",
      "-242.955276043\n",
      "-242.597202129\n",
      "-242.536776765\n",
      "-242.491025\n",
      "-242.450701843\n",
      "-242.951678335\n",
      "-242.582213106\n",
      "-242.563624413\n",
      "-242.515015825\n",
      "-242.465641219\n",
      "-242.960865674\n",
      "-242.718156234\n",
      "-242.612663285\n",
      "-242.5602876\n",
      "-242.509580049\n",
      "-242.467306994\n",
      "-242.955294818\n",
      "-242.598023654\n",
      "-242.537927409\n",
      "-242.49200437\n",
      "-242.451950784\n",
      "-242.951400032\n",
      "-242.583143513\n",
      "-242.563626043\n",
      "-242.515041628\n",
      "-242.476393523\n",
      "-242.440318548\n",
      "-242.955012356\n",
      "-242.591473272\n",
      "-242.565732809\n",
      "-242.557993332\n",
      "-242.508159824\n",
      "-242.459595896\n",
      "-242.959127623\n",
      "-242.716312211\n",
      "-242.612370616\n",
      "-242.559661784\n",
      "-242.507411663\n",
      "-242.465719256\n",
      "-242.955269163\n",
      "-242.597237532\n",
      "-242.53682245\n",
      "-242.491078734\n",
      "-242.450759703\n",
      "-242.95165652\n",
      "-242.58228991\n",
      "-242.563617702\n",
      "-242.51499822\n",
      "-242.465220993\n",
      "-242.960838584\n",
      "-242.718324145\n",
      "-242.612570506\n",
      "-242.560246778\n",
      "-242.509480664\n",
      "-242.467192186\n",
      "-242.955338479\n",
      "-242.597867306\n",
      "-242.537721133\n",
      "-242.491761243\n",
      "-242.451688418\n",
      "-242.95149904\n",
      "-242.582794473\n",
      "-242.563654186\n",
      "-242.515121049\n",
      "-242.476714545\n",
      "-242.440494858\n",
      "-242.954995214\n",
      "-242.591423371\n",
      "-242.565704752\n",
      "-242.557979334\n",
      "-242.508172909\n",
      "-242.459621133\n",
      "-242.959114347\n",
      "-242.71628099\n",
      "-242.612383899\n",
      "-242.559683872\n",
      "-242.507437037\n",
      "-242.465746368\n"
     ]
    }
   ],
   "source": [
    "SHMM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speed[['corr_cor']] = (speed[['corr']] == 'cor')*1\n",
    "speed[['corr_inc']] = (speed[['corr']] == 'inc')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SHMM = SupervisedHMM(num_states=2, max_EM_iter=1000, EM_tol=1e-4)\n",
    "SHMM.setData([speed])\n",
    "SHMM.setModels(model_emissions = [MNLP()], model_transition=MNLP(solver='lbfgs'))\n",
    "SHMM.setInputs(covariates_initial = [], covariates_transition = [], covariates_emissions = [[]])\n",
    "SHMM.setOutputs([['corr_cor', 'corr_inc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82657836  0.17342164]]\n",
      "[[ 0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "print np.exp(SHMM.model_transition[0].coef - logsumexp(SHMM.model_transition[0].coef))\n",
    "print np.exp(SHMM.model_transition[1].coef - logsumexp(SHMM.model_transition[1].coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.73920482]\n",
      "[ 1.66229956]\n",
      "[[ 0.         -2.61160866]]\n",
      "[[ 0.          1.39316674]]\n"
     ]
    }
   ],
   "source": [
    "print SHMM.model_emissions[0][0].coef\n",
    "print SHMM.model_emissions[1][0].coef\n",
    "print SHMM.model_emissions[0][1].coef\n",
    "print SHMM.model_emissions[1][1].coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SHMM = SupervisedHMM(num_states=2, max_EM_iter=1000, EM_tol=1e-4)\n",
    "SHMM.setData([speed])\n",
    "SHMM.setModels(model_emissions = [MNLP(), LM()], model_transition=MNLP(solver='lbfgs'))\n",
    "SHMM.setInputs(covariates_initial = [], covariates_transition = [], covariates_emissions = [[],[]])\n",
    "SHMM.setOutputs([['corr_cor', 'corr_inc'],['rt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-554.356505215\n",
      "-554.266412655\n",
      "-554.1568363\n",
      "-554.004421166\n",
      "-553.787395952\n",
      "-553.463924056\n",
      "-552.952769104\n",
      "-552.088138392\n",
      "-550.507553644\n",
      "-547.365116065\n",
      "-540.523964056\n",
      "-524.421840281\n",
      "-487.94724076\n",
      "-409.021873913\n",
      "-321.694639134\n",
      "-302.425144008\n",
      "-304.645575419\n",
      "-304.630330904\n",
      "-304.648839786\n",
      "-304.666728735\n",
      "-304.682945309\n",
      "-304.695464385\n",
      "-304.704451202\n",
      "-304.710673101\n",
      "-304.714896562\n",
      "-304.717730763\n",
      "-304.719619492\n",
      "-304.720872697\n",
      "-304.721701929\n",
      "-304.722249648\n",
      "-304.722611007\n",
      "-304.722849237\n",
      "-304.723006215\n",
      "-304.72310962\n",
      "-304.723177721\n"
     ]
    }
   ],
   "source": [
    "SHMM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.90881261  0.09118739]]\n",
      "[[ 0.19666372  0.80333628]]\n"
     ]
    }
   ],
   "source": [
    "print np.exp(SHMM.model_transition[0].coef - logsumexp(SHMM.model_transition[0].coef))\n",
    "print np.exp(SHMM.model_transition[1].coef - logsumexp(SHMM.model_transition[1].coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.38720339]\n",
      "[ 5.51214045]\n"
     ]
    }
   ],
   "source": [
    "print SHMM.model_emissions[0][1].coef\n",
    "print SHMM.model_emissions[1][1].coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -2.18085911]]\n",
      "[[ 0.         -0.10319709]]\n"
     ]
    }
   ],
   "source": [
    "print SHMM.model_emissions[0][0].coef\n",
    "print SHMM.model_emissions[1][0].coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speed['rt_int'] = np.floor(speed['rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SHMM = SupervisedHMM(num_states=2, max_EM_iter=1000, EM_tol=1e-4)\n",
    "SHMM.setData([speed])\n",
    "SHMM.setModels(model_emissions = [GLM(fam = family.Poisson())], model_transition=MNLP(solver='lbfgs'))\n",
    "SHMM.setInputs(covariates_initial = [], covariates_transition = [], covariates_emissions = [[]])\n",
    "SHMM.setOutputs([['rt_int']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-797.004237043\n",
      "-797.003797161\n",
      "-797.003796486\n"
     ]
    }
   ],
   "source": [
    "SHMM.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
